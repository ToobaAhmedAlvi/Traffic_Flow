{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c8bd4e7-b6a9-4ee7-9518-a5e4983463c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VehicleID</th>\n",
       "      <th>PlateNumber</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>OwnerID</th>\n",
       "      <th>PlateNumber_Cleaned</th>\n",
       "      <th>VehicleType_Cleaned</th>\n",
       "      <th>VehicleType_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>92X 5338</td>\n",
       "      <td>Car</td>\n",
       "      <td>64</td>\n",
       "      <td>92X5338</td>\n",
       "      <td>Car</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>186-ZJJ</td>\n",
       "      <td>Car</td>\n",
       "      <td>145</td>\n",
       "      <td>186ZJJ</td>\n",
       "      <td>Car</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>835 1WK</td>\n",
       "      <td>Car</td>\n",
       "      <td>140</td>\n",
       "      <td>8351WK</td>\n",
       "      <td>Car</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>OYD7711</td>\n",
       "      <td>Bus</td>\n",
       "      <td>70</td>\n",
       "      <td>OYD7711</td>\n",
       "      <td>Bus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>KFU-0955</td>\n",
       "      <td>Bus</td>\n",
       "      <td>24</td>\n",
       "      <td>KFU0955</td>\n",
       "      <td>Bus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VehicleID PlateNumber VehicleType  OwnerID PlateNumber_Cleaned  \\\n",
       "0          1    92X 5338         Car       64             92X5338   \n",
       "1          2     186-ZJJ         Car      145              186ZJJ   \n",
       "2          3     835 1WK         Car      140              8351WK   \n",
       "3          4     OYD7711         Bus       70             OYD7711   \n",
       "4          5    KFU-0955         Bus       24             KFU0955   \n",
       "\n",
       "  VehicleType_Cleaned  VehicleType_Encoded  \n",
       "0                 Car                    1  \n",
       "1                 Car                    1  \n",
       "2                 Car                    1  \n",
       "3                 Bus                    2  \n",
       "4                 Bus                    2  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both sheets FIRST\n",
    "df_vehicles = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Vehicles', engine='openpyxl')\n",
    "df_drivers = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Drivers', engine='openpyxl')\n",
    "df_speed_violations = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='SpeedViolations', engine='openpyxl')\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "1bafc649-9aec-4039-84db-844d9cc07a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VehicleID</th>\n",
       "      <th>PlateNumber</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>OwnerID</th>\n",
       "      <th>PlateNumber_Cleaned</th>\n",
       "      <th>VehicleType_Cleaned</th>\n",
       "      <th>VehicleType_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>9MR 974</td>\n",
       "      <td>Car</td>\n",
       "      <td>112</td>\n",
       "      <td>9MR974</td>\n",
       "      <td>Car</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>AI-9234</td>\n",
       "      <td>Bus</td>\n",
       "      <td>60</td>\n",
       "      <td>AI9234</td>\n",
       "      <td>Bus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>6538</td>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>72</td>\n",
       "      <td>6538</td>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>XPA6114</td>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>128</td>\n",
       "      <td>XPA6114</td>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>14K X29</td>\n",
       "      <td>Car</td>\n",
       "      <td>39</td>\n",
       "      <td>14KX29</td>\n",
       "      <td>Car</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     VehicleID PlateNumber VehicleType  OwnerID PlateNumber_Cleaned  \\\n",
       "195        196     9MR 974         Car      112              9MR974   \n",
       "196        197     AI-9234         Bus       60              AI9234   \n",
       "197        198        6538  Motorcycle       72                6538   \n",
       "198        199     XPA6114  Motorcycle      128             XPA6114   \n",
       "199        200     14K X29         Car       39              14KX29   \n",
       "\n",
       "    VehicleType_Cleaned  VehicleType_Encoded  \n",
       "195                 Car                    1  \n",
       "196                 Bus                    2  \n",
       "197          Motorcycle                    4  \n",
       "198          Motorcycle                    4  \n",
       "199                 Car                    1  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897a57a-cb74-4496-b0b9-89db552afee9",
   "metadata": {},
   "source": [
    "## DIMENSION TABLE CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf7dd5-96f8-4eb7-aaed-65c4c638133c",
   "metadata": {},
   "source": [
    "##### 1. VehicleID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b34cd6f-c189-4cb1-a8ce-a994b40c5aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing VehicleID values: 0\n",
      "Duplicate VehicleID values: 0\n",
      "Final check — are VehicleIDs unique?: True\n"
     ]
    }
   ],
   "source": [
    "#1) Missing values\n",
    "missing_vehicle_ids = df['VehicleID'].isnull().sum()\n",
    "print(f\"Missing VehicleID values: {missing_vehicle_ids}\")\n",
    "\n",
    "#2) Duplicates\n",
    "duplicate_vehicle_ids = df['VehicleID'].duplicated().sum()\n",
    "print(f\"Duplicate VehicleID values: {duplicate_vehicle_ids}\")\n",
    "\n",
    "#3) If data type is integer & convert if not:\n",
    "if df['VehicleID'].dtype != 'int64':\n",
    "    df['VehicleID'] = pd.to_numeric(df['VehicleID'], errors='coerce')\n",
    "    print(\"Converted VehicleID to integer.\")\n",
    "\n",
    "#4) Droping rows w/ missing or duplicate values (in case)\n",
    "df_cleaned = df.dropna(subset=['VehicleID'])\n",
    "df_cleaned = df_cleaned[~df_cleaned['VehicleID'].duplicated()]\n",
    "\n",
    "#5) Final confirmation\n",
    "print(\"Final check — are VehicleIDs unique?:\", df_cleaned['VehicleID'].is_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47589568-1b79-4d85-9002-24f3081c3b64",
   "metadata": {},
   "source": [
    "##### 2. Plate number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9d7e0e94-7745-4d6f-a131-35f433695801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing PlateNumber values: 0\n",
      "  PlateNumber PlateNumber_Cleaned\n",
      "0    92X 5338             92X5338\n",
      "1     186-ZJJ              186ZJJ\n",
      "2     835 1WK              8351WK\n",
      "3     OYD7711             OYD7711\n",
      "4    KFU-0955             KFU0955\n",
      "5     41T•740              41T740\n",
      "6     93P L10              93PL10\n",
      "7    0N H8535             0NH8535\n",
      "8     Q81 4GN              Q814GN\n",
      "9     049-PVL              049PVL\n"
     ]
    }
   ],
   "source": [
    "#1) Checking for missing values\n",
    "missing_plates = df['PlateNumber'].isnull().sum()\n",
    "print(f\"Missing PlateNumber values: {missing_plates}\")\n",
    "\n",
    "#2) Checking if clean & standardize PlateNumber\n",
    "def clean_plate(plate):\n",
    "    if pd.isnull(plate):\n",
    "        return \"UNKNOWN\"\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9]', '', str(plate))  # Remove special characters\n",
    "    return cleaned.upper()  # Standardize to uppercase\n",
    "\n",
    "#3) Cleaning function\n",
    "df['PlateNumber_Cleaned'] = df['PlateNumber'].apply(clean_plate)\n",
    "\n",
    "#4) See some before/after examples for sample\n",
    "print(df[['PlateNumber', 'PlateNumber_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905510b9-0c97-4c2c-b813-b7325dfc4dbe",
   "metadata": {},
   "source": [
    "##### 3. Vehicle type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "84151252-9d4e-4574-887e-1c2451197aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original VehicleType values:\n",
      "VehicleType\n",
      "Truck         64\n",
      "Motorcycle    48\n",
      "Bus           45\n",
      "Car           43\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cleaned VehicleType values:\n",
      "VehicleType_Cleaned\n",
      "Truck         64\n",
      "Motorcycle    48\n",
      "Bus           45\n",
      "Car           43\n",
      "Name: count, dtype: int64\n",
      "  VehicleType VehicleType_Cleaned  VehicleType_Encoded\n",
      "0         Car                 Car                    1\n",
      "1         Car                 Car                    1\n",
      "2         Car                 Car                    1\n",
      "3         Bus                 Bus                    2\n",
      "4         Bus                 Bus                    2\n",
      "5  Motorcycle          Motorcycle                    4\n",
      "6  Motorcycle          Motorcycle                    4\n",
      "7         Car                 Car                    1\n",
      "8       Truck               Truck                    3\n",
      "9       Truck               Truck                    3\n"
     ]
    }
   ],
   "source": [
    "#1) Checking unique values before cleaning\n",
    "print(\"Original VehicleType values:\")\n",
    "print(df['VehicleType'].value_counts(dropna=False))\n",
    "\n",
    "#2) Cleaning and standardize the categories\n",
    "def clean_vehicle_type(vtype):\n",
    "    if pd.isnull(vtype):\n",
    "        return \"Unknown\"\n",
    "    vtype = str(vtype).strip().lower()\n",
    "    if \"car\" in vtype:\n",
    "        return \"Car\"\n",
    "    elif \"bus\" in vtype:\n",
    "        return \"Bus\"\n",
    "    elif \"truck\" in vtype:\n",
    "        return \"Truck\"\n",
    "    elif \"motor\" in vtype or \"bike\" in vtype:\n",
    "        return \"Motorcycle\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "df['VehicleType_Cleaned'] = df['VehicleType'].apply(clean_vehicle_type)\n",
    "\n",
    "#3) Encoding categories to integers\n",
    "vehicle_type_encoding = {\n",
    "    \"Car\": 1,\n",
    "    \"Bus\": 2,\n",
    "    \"Truck\": 3,\n",
    "    \"Motorcycle\": 4,\n",
    "    \"Other\": 5,\n",
    "    \"Unknown\": 0\n",
    "}\n",
    "df['VehicleType_Encoded'] = df['VehicleType_Cleaned'].map(vehicle_type_encoding)\n",
    "\n",
    "#4) Check cleaned distribution\n",
    "print(\"\\nCleaned VehicleType values:\")\n",
    "print(df['VehicleType_Cleaned'].value_counts())\n",
    "\n",
    "#5) Preview results\n",
    "print(df[['VehicleType', 'VehicleType_Cleaned', 'VehicleType_Encoded']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be7149-6092-4e9d-9d0b-00b8828bd1cb",
   "metadata": {},
   "source": [
    "##### 4. Owner ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a295832f-b6ed-456e-a6d0-afc943986def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing OwnerID values: 0\n",
      "\n",
      "OwnerIDs with no matching DriverID:\n",
      "Empty DataFrame\n",
      "Columns: [VehicleID, OwnerID]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#1) Checking for missing OwnerIDs\n",
    "missing_owners = df_vehicles['OwnerID'].isnull().sum()\n",
    "print(f\"Missing OwnerID values: {missing_owners}\")\n",
    "\n",
    "#2) Getting all valid DriverIDs\n",
    "valid_driver_ids = set(df_drivers['DriverID'])\n",
    "\n",
    "#3) Checking which OwnerIDs in Vehicles are not found in Drivers\n",
    "df_vehicles['OwnerID_Invalid'] = ~df_vehicles['OwnerID'].isin(valid_driver_ids)\n",
    "invalid_owner_rows = df_vehicles[df_vehicles['OwnerID_Invalid']]\n",
    "\n",
    "#4) Printing the invalid OwnerIDs\n",
    "print(\"\\nOwnerIDs with no matching DriverID:\")\n",
    "print(invalid_owner_rows[['VehicleID', 'OwnerID']])\n",
    "\n",
    "#5) Fix invalid OwnerIDs by assigning a default or dropping them\n",
    "# Example: Replace with -1 or 'Unknown'\n",
    "df_vehicles['OwnerID_Cleaned'] = df_vehicles.apply(\n",
    "    lambda row: row['OwnerID'] if not row['OwnerID_Invalid'] else -1,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "### Note: Thus, referential integrity is being maintained in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52d0ef-5b1f-410e-b34e-e43fee6085fb",
   "metadata": {},
   "source": [
    "##### 5. DriverID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6236d74a-baee-40da-a594-955422d0222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing DriverID values: 0\n",
      "Duplicate DriverID values: 0\n",
      "Are DriverIDs unique after cleaning?: True\n"
     ]
    }
   ],
   "source": [
    "#1) Check for missing values\n",
    "missing_driver_ids = df_drivers['DriverID'].isnull().sum()\n",
    "print(f\"Missing DriverID values: {missing_driver_ids}\")\n",
    "\n",
    "#2) Check for duplicates\n",
    "duplicate_driver_ids = df_drivers['DriverID'].duplicated().sum()\n",
    "print(f\"Duplicate DriverID values: {duplicate_driver_ids}\")\n",
    "\n",
    "#3) Ensure the data type is integer\n",
    "if not pd.api.types.is_integer_dtype(df_drivers['DriverID']):\n",
    "    df_drivers['DriverID'] = pd.to_numeric(df_drivers['DriverID'], errors='coerce').astype('Int64')\n",
    "    print(\"DriverID converted to integer type.\")\n",
    "\n",
    "#4) Drop rows with missing or duplicate IDs (optional)\n",
    "df_cleaned_drivers = df_drivers.dropna(subset=['DriverID'])\n",
    "df_cleaned_drivers = df_cleaned_drivers[~df_cleaned_drivers['DriverID'].duplicated()]\n",
    "\n",
    "#5) Final confirmation\n",
    "print(\"Are DriverIDs unique after cleaning?:\", df_cleaned_drivers['DriverID'].is_unique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda8f18-b7cc-466e-8099-dab85cb83717",
   "metadata": {},
   "source": [
    "##### 6. Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e0287968-eba6-4d06-9135-9c9eead3c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Name values: 0\n",
      "              Name     Name_Cleaned\n",
      "0   Muhammad Ahmed   Muhammad Ahmed\n",
      "1       Ali Hassan       Ali Hassan\n",
      "2  Kamran Siddiqui  Kamran Siddiqui\n",
      "3      Bilal Iqbal      Bilal Iqbal\n",
      "4      Usman Malik      Usman Malik\n",
      "5     Farhan Ahmed     Farhan Ahmed\n",
      "6       Imran Raza       Imran Raza\n",
      "7       Asif Iqbal       Asif Iqbal\n",
      "8    Zubair Sheikh    Zubair Sheikh\n",
      "9       Adnan Khan       Adnan Khan\n"
     ]
    }
   ],
   "source": [
    "#1) Checking for missing values\n",
    "missing_names = df_drivers['Name'].isnull().sum()\n",
    "print(f\"Missing Name values: {missing_names}\")\n",
    "\n",
    "#2) Function to clean and title-case names\n",
    "def clean_name(name):\n",
    "    if pd.isnull(name):\n",
    "        return \"Unknown\"\n",
    "    return str(name).strip().title()  # Removes leading/trailing spaces & capitalizes each word\n",
    "\n",
    "#3) Applying the function\n",
    "df_drivers['Name_Cleaned'] = df_drivers['Name'].apply(clean_name)\n",
    "\n",
    "#5) Show some before-and-after examples\n",
    "print(df_drivers[['Name', 'Name_Cleaned']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f88428-cc6a-4b9d-b054-4e5ccf6c9c36",
   "metadata": {},
   "source": [
    "##### 7. Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b78f447a-f8f6-4449-bc54-072fdf6b4248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Age values: 0\n",
      "Invalid Age values (less than 18 or more than 100):\n",
      "Empty DataFrame\n",
      "Columns: [DriverID, Age]\n",
      "Index: []\n",
      "\n",
      "Cleaned Age values:\n",
      "    DriverID  Age  Age_Cleaned\n",
      "0         1   56           56\n",
      "1         2   24           24\n",
      "2         3   25           25\n",
      "3         4   42           42\n",
      "4         5   62           62\n",
      "5         6   60           60\n",
      "6         7   58           58\n",
      "7         8   59           59\n",
      "8         9   26           26\n",
      "9        10   56           56\n"
     ]
    }
   ],
   "source": [
    "#1) Missing values in the Age col\n",
    "missing_age = df_drivers['Age'].isnull().sum()\n",
    "print(f\"Missing Age values: {missing_age}\")\n",
    "\n",
    "#2) Invalid ages (negative values or unrealistic ones, like > 100)\n",
    "invalid_ages = df_drivers[(df_drivers['Age'] < 18) | (df_drivers['Age'] > 100)]\n",
    "print(f\"Invalid Age values (less than 18 or more than 100):\\n{invalid_ages[['DriverID', 'Age']]}\")\n",
    "\n",
    "#3) Now replacing invalid ages w/ the median or a default value\n",
    "median_age = df_drivers['Age'].median()\n",
    "df_drivers['Age_Cleaned'] = df_drivers['Age'].apply(\n",
    "    lambda x: median_age if x < 18 or x > 100 else x\n",
    ")\n",
    "\n",
    "#4) Verifying it now\n",
    "print(\"\\nCleaned Age values:\\n\", df_drivers[['DriverID', 'Age', 'Age_Cleaned']].head(10))\n",
    "\n",
    "#5) (Optional) Visualize or return the cleaned data for review\n",
    "# df_drivers.to_excel(\"Cleaned_Drivers_Age.xlsx\", index=False)  # Uncomment to save cleaned data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07f370-2b1a-4ca8-a55e-077dbcaa9e5a",
   "metadata": {},
   "source": [
    "##### 8. License number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fa85a5ea-2049-4286-9579-cf1f21a6698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing LicenseNumber values: 0\n",
      "Duplicate LicenseNumber values: 0\n",
      "\n",
      "Duplicate LicenseNumber rows:\n",
      "Empty DataFrame\n",
      "Columns: [DriverID, LicenseNumber]\n",
      "Index: []\n",
      "Duplicate LicenseNumber values after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "#1) Missing LicenseNumber values\n",
    "missing_license_numbers = df_drivers['LicenseNumber'].isnull().sum()\n",
    "print(f\"Missing LicenseNumber values: {missing_license_numbers}\")\n",
    "\n",
    "#2) duplicate LicenseNumber values\n",
    "duplicate_license_numbers = df_drivers['LicenseNumber'].duplicated().sum()\n",
    "print(f\"Duplicate LicenseNumber values: {duplicate_license_numbers}\")\n",
    "\n",
    "#3) List duplicate LicenseNumbers & their associated DriverIDs\n",
    "duplicate_rows = df_drivers[df_drivers['LicenseNumber'].duplicated()]\n",
    "print(\"\\nDuplicate LicenseNumber rows:\")\n",
    "print(duplicate_rows[['DriverID', 'LicenseNumber']])\n",
    "\n",
    "#4) Removing duplicates or handle them\n",
    "df_drivers_cleaned = df_drivers.drop_duplicates(subset=['LicenseNumber'], keep='first')\n",
    "\n",
    "#5) Confirming no more duplicates\n",
    "duplicate_license_numbers_after_clean = df_drivers_cleaned['LicenseNumber'].duplicated().sum()\n",
    "print(f\"Duplicate LicenseNumber values after cleaning: {duplicate_license_numbers_after_clean}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae9a9a-74ce-406c-a841-59af88f416b2",
   "metadata": {},
   "source": [
    "##### 9. Experience Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2d2df058-776f-4471-9377-d3dd52d207bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing ExperienceYears values: 0\n",
      "Missing Age values: 0\n",
      "\n",
      "Invalid ExperienceYears (greater than Age):\n",
      "     DriverID  Age  ExperienceYears\n",
      "2           3   25               42\n",
      "3           4   42               43\n",
      "11         12   30               40\n",
      "19         20   23               34\n",
      "21         22   21               30\n",
      "37         38   33               47\n",
      "64         65   20               24\n",
      "84         85   27               44\n",
      "88         89   29               42\n",
      "94         95   20               38\n",
      "98         99   21               28\n",
      "99        100   34               40\n",
      "103       104   33               46\n",
      "110       111   23               38\n",
      "114       115   19               39\n",
      "117       118   18               29\n",
      "129       130   26               38\n",
      "140       141   29               35\n",
      "162       163   33               42\n",
      "165       166   45               49\n",
      "170       171   28               39\n",
      "171       172   30               50\n",
      "177       178   35               37\n",
      "181       182   18               32\n",
      "188       189   19               39\n",
      "192       193   31               40\n",
      "193       194   33               38\n",
      "199       200   41               50\n",
      "\n",
      "Cleaned ExperienceYears values (ExperienceYears ≤ Age):\n",
      "   DriverID  Age  ExperienceYears  ExperienceYears_Cleaned\n",
      "0         1   56               42                       42\n",
      "1         2   24               10                       10\n",
      "2         3   25               42                       25\n",
      "3         4   42               43                       42\n",
      "4         5   62               41                       41\n",
      "5         6   60               15                       15\n",
      "6         7   58               30                       30\n",
      "7         8   59                4                        4\n",
      "8         9   26               25                       25\n",
      "9        10   56                2                        2\n"
     ]
    }
   ],
   "source": [
    "#1)Missing values in the ExperienceYears and Age columns\n",
    "missing_experience = df_drivers['ExperienceYears'].isnull().sum()\n",
    "print(f\"Missing ExperienceYears values: {missing_experience}\")\n",
    "missing_age = df_drivers['Age'].isnull().sum()\n",
    "print(f\"Missing Age values: {missing_age}\")\n",
    "\n",
    "#2) ExperienceYears is greater than Age (invalid cases)\n",
    "invalid_experience = df_drivers[df_drivers['ExperienceYears'] > df_drivers['Age']]\n",
    "print(f\"\\nInvalid ExperienceYears (greater than Age):\\n{invalid_experience[['DriverID', 'Age', 'ExperienceYears']]}\")\n",
    "\n",
    "#3) Replacing invalid ExperienceYears with Age (or handle it in another way)\n",
    "df_drivers['ExperienceYears_Cleaned'] = df_drivers.apply(\n",
    "    lambda row: row['Age'] if row['ExperienceYears'] > row['Age'] else row['ExperienceYears'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#5) Confirm the cleaning process\n",
    "print(\"\\nCleaned ExperienceYears values (ExperienceYears ≤ Age):\")\n",
    "print(df_drivers[['DriverID', 'Age', 'ExperienceYears', 'ExperienceYears_Cleaned']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c1045-d4c1-4c97-a06d-22551858f5fb",
   "metadata": {},
   "source": [
    "##### 10. Violation ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "181b925f-9b98-4cf4-8907-eede3608e3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing ViolationID: 0\n",
      "Duplicate ViolationID: 0\n",
      "\n",
      "Duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [ViolationID, VehicleID]\n",
      "Index: []\n",
      "Final duplicate check: 0\n"
     ]
    }
   ],
   "source": [
    "# 1) Checking for missing ViolationID values\n",
    "missing_violation_ids = df_speed_violations['ViolationID'].isnull().sum()\n",
    "print(f\"Missing ViolationID: {missing_violation_ids}\")\n",
    "\n",
    "# 2) Checking for duplicates\n",
    "duplicate_violation_ids = df_speed_violations['ViolationID'].duplicated().sum()\n",
    "print(f\"Duplicate ViolationID: {duplicate_violation_ids}\")\n",
    "\n",
    "# 3) Showing duplicate rows\n",
    "duplicate_rows = df_speed_violations[df_speed_violations['ViolationID'].duplicated()]\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(duplicate_rows[['ViolationID', 'VehicleID']])\n",
    "\n",
    "# 4) Dropping duplicates and keeping the first\n",
    "df_speed_violations_cleaned = df_speed_violations.drop_duplicates(subset=['ViolationID'], keep='first')\n",
    "\n",
    "# 5) Final check for duplicates\n",
    "final_duplicate_check = df_speed_violations_cleaned['ViolationID'].duplicated().sum()\n",
    "print(f\"Final duplicate check: {final_duplicate_check}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598cc66c-9dfc-449d-8f58-d5b7151d7f5f",
   "metadata": {},
   "source": [
    "##### 11. Vehicle ID [refrential integrity check with Speed Violations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4df524cf-87c3-4368-b3c3-dbbfb94e27d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid VehicleIDs (not in Vehicles table):\n",
      "Empty DataFrame\n",
      "Columns: [ViolationID, VehicleID]\n",
      "Index: []\n",
      "Rows with invalid VehicleID after cleaning: Empty DataFrame\n",
      "Columns: [ViolationID, VehicleID_Cleaned]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 1) Checking if all VehicleIDs in SpeedViolations exist in Vehicles\n",
    "invalid_vehicle_ids = df_speed_violations[~df_speed_violations['VehicleID'].isin(df_vehicles['VehicleID'])]\n",
    "print(f\"Invalid VehicleIDs (not in Vehicles table):\\n{invalid_vehicle_ids[['ViolationID', 'VehicleID']]}\")\n",
    "\n",
    "# 2) Handle missing or invalid VehicleIDs (optional)\n",
    "# Replace invalid VehicleIDs with a placeholder (e.g., -1) or flag them for review\n",
    "df_speed_violations['VehicleID_Cleaned'] = df_speed_violations['VehicleID'].apply(\n",
    "    lambda x: x if x in df_vehicles['VehicleID'].values else -1\n",
    ")\n",
    "\n",
    "# 3) Final check to ensure all VehicleIDs are valid\n",
    "final_invalid_check = df_speed_violations[df_speed_violations['VehicleID_Cleaned'] == -1]\n",
    "print(f\"Rows with invalid VehicleID after cleaning: {final_invalid_check[['ViolationID', 'VehicleID_Cleaned']].head(10)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55a487-da49-43c8-bbf8-7cba9ea4d015",
   "metadata": {},
   "source": [
    "##### 12. Speed Recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e02c7e56-276a-490c-a752-af4dab06a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative speed values:\n",
      "Empty DataFrame\n",
      "Columns: [ViolationID, SpeedRecorded]\n",
      "Index: []\n",
      "Speed outliers (greater than 200):\n",
      "Empty DataFrame\n",
      "Columns: [ViolationID, SpeedRecorded]\n",
      "Index: []\n",
      "Cleaned speed values:\n",
      "   ViolationID  SpeedRecorded  SpeedRecorded_Cleaned\n",
      "0            1            140                    140\n",
      "1            2            105                    105\n",
      "2            3             92                     92\n",
      "3            4            120                    120\n",
      "4            5            113                    113\n",
      "5            6            134                    134\n",
      "6            7            124                    124\n",
      "7            8            103                    103\n",
      "8            9             65                     65\n",
      "9           10            107                    107\n"
     ]
    }
   ],
   "source": [
    "# 1) Checking for negative speed values\n",
    "negative_speeds = df_speed_violations[df_speed_violations['SpeedRecorded'] < 0]\n",
    "print(f\"Negative speed values:\\n{negative_speeds[['ViolationID', 'SpeedRecorded']]}\")\n",
    "\n",
    "# 2) Replace negative speeds with the median or a default value (e.g., 0)\n",
    "median_speed = df_speed_violations['SpeedRecorded'].median()\n",
    "df_speed_violations['SpeedRecorded_Cleaned'] = df_speed_violations['SpeedRecorded'].apply(\n",
    "    lambda x: x if x >= 0 else median_speed\n",
    ")\n",
    "\n",
    "# 3) Optional: Check for outliers (e.g., speeds > 200) and handle them\n",
    "outliers = df_speed_violations[df_speed_violations['SpeedRecorded'] > 200]\n",
    "print(f\"Speed outliers (greater than 200):\\n{outliers[['ViolationID', 'SpeedRecorded']]}\")\n",
    "\n",
    "# 4) Replace outliers with the upper limit or a predefined value (e.g., 200)\n",
    "df_speed_violations['SpeedRecorded_Cleaned'] = df_speed_violations['SpeedRecorded_Cleaned'].apply(\n",
    "    lambda x: 200 if x > 200 else x\n",
    ")\n",
    "\n",
    "# 5) Final check for cleaned speeds\n",
    "print(f\"Cleaned speed values:\\n{df_speed_violations[['ViolationID', 'SpeedRecorded', 'SpeedRecorded_Cleaned']].head(10)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09549971-3841-48ea-abb6-8a156269e7f8",
   "metadata": {},
   "source": [
    "##### 13. Speed Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9460808a-56e3-49f7-bbdf-a97edc1eff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid SpeedLimit values (outside of 10-150 range):\n",
      "Empty DataFrame\n",
      "Columns: [ViolationID, SpeedLimit]\n",
      "Index: []\n",
      "High SpeedLimit values (above 130):\n",
      "Empty DataFrame\n",
      "Columns: [ViolationID, SpeedLimit]\n",
      "Index: []\n",
      "Cleaned SpeedLimit values:\n",
      "   ViolationID  SpeedLimit  SpeedLimit_Cleaned\n",
      "0            1          80                  80\n",
      "1            2          80                  80\n",
      "2            3          70                  70\n",
      "3            4          70                  70\n",
      "4            5          70                  70\n",
      "5            6          70                  70\n",
      "6            7          90                  90\n",
      "7            8          50                  50\n",
      "8            9          60                  60\n",
      "9           10          80                  80\n"
     ]
    }
   ],
   "source": [
    "# 1) Checking if SpeedLimit values are within a reasonable range (e.g., 10 to 150 km/h)\n",
    "invalid_speed_limits = df_speed_violations[(df_speed_violations['SpeedLimit'] < 10) | (df_speed_violations['SpeedLimit'] > 150)]\n",
    "print(f\"Invalid SpeedLimit values (outside of 10-150 range):\\n{invalid_speed_limits[['ViolationID', 'SpeedLimit']]}\")\n",
    "\n",
    "# 2) Handle invalid SpeedLimit values by replacing them with a default (e.g., 50)\n",
    "df_speed_violations['SpeedLimit_Cleaned'] = df_speed_violations['SpeedLimit'].apply(\n",
    "    lambda x: 50 if x < 10 or x > 150 else x\n",
    ")\n",
    "\n",
    "# 3) Optional: Check for unusually high SpeedLimit values (e.g., above 130 km/h in residential zones)\n",
    "high_speed_limits = df_speed_violations[df_speed_violations['SpeedLimit'] > 130]\n",
    "print(f\"High SpeedLimit values (above 130):\\n{high_speed_limits[['ViolationID', 'SpeedLimit']]}\")\n",
    "\n",
    "# 4) Handle high SpeedLimit values by capping them at 130 km/h (or a suitable limit)\n",
    "df_speed_violations['SpeedLimit_Cleaned'] = df_speed_violations['SpeedLimit_Cleaned'].apply(\n",
    "    lambda x: 130 if x > 130 else x\n",
    ")\n",
    "\n",
    "# 5) Final check for cleaned SpeedLimit values\n",
    "print(f\"Cleaned SpeedLimit values:\\n{df_speed_violations[['ViolationID', 'SpeedLimit', 'SpeedLimit_Cleaned']].head(10)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd82ee-76e7-4507-9d1e-84021fcc113b",
   "metadata": {},
   "source": [
    "##### 14. Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d3060296-5458-4300-a68c-66d00f2a1731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Timestamp values:\n",
      "Empty DataFrame\n",
      "Columns: [ViolationID, Timestamp]\n",
      "Index: []\n",
      "Timestamps out of range:\n",
      "Empty DataFrame\n",
      "Columns: [ViolationID, Timestamp]\n",
      "Index: []\n",
      "Cleaned Timestamp values:\n",
      "   ViolationID           Timestamp   Timestamp_Cleaned\n",
      "0            1 2025-02-12 16:54:07 2025-02-12 16:54:07\n",
      "1            2 2025-02-23 02:07:01 2025-02-23 02:07:01\n",
      "2            3 2025-03-03 09:24:10 2025-03-03 09:24:10\n",
      "3            4 2025-02-03 03:45:09 2025-02-03 03:45:09\n",
      "4            5 2025-02-14 06:10:16 2025-02-14 06:10:16\n",
      "5            6 2025-01-06 10:42:22 2025-01-06 10:42:22\n",
      "6            7 2025-01-24 00:45:49 2025-01-24 00:45:49\n",
      "7            8 2025-01-04 00:30:56 2025-01-04 00:30:56\n",
      "8            9 2025-02-24 09:56:41 2025-02-24 09:56:41\n",
      "9           10 2025-02-19 01:43:15 2025-02-19 01:43:15\n"
     ]
    }
   ],
   "source": [
    "# 1) Checking if the Timestamp is in a valid datetime format\n",
    "invalid_timestamp = pd.to_datetime(df_speed_violations['Timestamp'], errors='coerce')\n",
    "invalid_timestamps = df_speed_violations[invalid_timestamp.isna()]\n",
    "print(f\"Invalid Timestamp values:\\n{invalid_timestamps[['ViolationID', 'Timestamp']]}\")\n",
    "\n",
    "# 2) If invalid, replace with the current timestamp\n",
    "df_speed_violations['Timestamp_Cleaned'] = df_speed_violations['Timestamp'].apply(\n",
    "    lambda x: pd.Timestamp('today') if pd.to_datetime(x, errors='coerce') is pd.NaT else x\n",
    ")\n",
    "\n",
    "# 3) Fixing the range check - 10 years ago from today\n",
    "start_date = pd.Timestamp.today() - pd.DateOffset(years=10)  # Correct way to get 10 years ago\n",
    "end_date = pd.Timestamp.today()\n",
    "\n",
    "# Checking for timestamps outside the valid range\n",
    "out_of_range_timestamps = df_speed_violations[\n",
    "    (pd.to_datetime(df_speed_violations['Timestamp']) < start_date) |\n",
    "    (pd.to_datetime(df_speed_violations['Timestamp']) > end_date)\n",
    "]\n",
    "print(f\"Timestamps out of range:\\n{out_of_range_timestamps[['ViolationID', 'Timestamp']]}\")\n",
    "\n",
    "# 4) If out of range, replace with the current date\n",
    "df_speed_violations['Timestamp_Cleaned'] = df_speed_violations['Timestamp_Cleaned'].apply(\n",
    "    lambda x: pd.Timestamp('today') if (pd.to_datetime(x) < start_date or pd.to_datetime(x) > end_date) else x\n",
    ")\n",
    "\n",
    "# 5) Final check for cleaned Timestamps\n",
    "print(f\"Cleaned Timestamp values:\\n{df_speed_violations[['ViolationID', 'Timestamp', 'Timestamp_Cleaned']].head(10)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b2cb5-1145-4712-9b9f-d972240af138",
   "metadata": {},
   "source": [
    "##### 15. Accident ID : Ensure uniqueness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "50baf4aa-6fe8-40c6-9bea-110794d672dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate AccidentID values: 0\n",
      "\n",
      "Duplicate AccidentID rows:\n",
      "Empty DataFrame\n",
      "Columns: [AccidentID, Location]\n",
      "Index: []\n",
      "Duplicate AccidentID values after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the Accidents sheet\n",
    "try:\n",
    "    df_accidents\n",
    "except NameError:\n",
    "    df_accidents = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Accidents', engine='openpyxl')\n",
    "\n",
    "# 2) Check how many duplicate AccidentIDs we have\n",
    "duplicate_accident_ids = df_accidents['AccidentID'].duplicated().sum()\n",
    "print(f\"Duplicate AccidentID values: {duplicate_accident_ids}\")\n",
    "\n",
    "# 3) Show rows with duplicate AccidentIDs so we can see what's wrong\n",
    "duplicate_rows = df_accidents[df_accidents['AccidentID'].duplicated()]\n",
    "print(\"\\nDuplicate AccidentID rows:\")\n",
    "print(duplicate_rows[['AccidentID', 'Location']])\n",
    "\n",
    "# 4) Drop duplicates by keeping the first occurrence of each AccidentID\n",
    "df_accidents_cleaned = df_accidents.drop_duplicates(subset=['AccidentID'], keep='first')\n",
    "\n",
    "# 5) Do a final check to confirm no duplicates remain\n",
    "final_duplicate_check = df_accidents_cleaned['AccidentID'].duplicated().sum()\n",
    "print(f\"Duplicate AccidentID values after cleaning: {final_duplicate_check}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a03e1-d0b2-4d16-9301-1c270f5e0cdf",
   "metadata": {},
   "source": [
    "##### 16. Location\tAccident:\tGeocode or standardize\t\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b4565f57-d463-4664-8da4-b2ce66b6eb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Location values: 0\n",
      "                Location       Location_Cleaned\n",
      "0  Shahrah-e-Bandar Road  Shahrah-E-Bandar Road\n",
      "1      Shahrah-e-Gulberg      Shahrah-E-Gulberg\n",
      "2        Miran Shah Road        Miran Shah Road\n",
      "3        Shahrah-e-PECHS        Shahrah-E-Pechs\n",
      "4    Sharae Quaid-e-Azam    Sharae Quaid-E-Azam\n",
      "5       Shahrah-e-Manora       Shahrah-E-Manora\n",
      "6    Shahrah-e-Saeedabad    Shahrah-E-Saeedabad\n",
      "7   Shahrah-e-Ibn-e-Sina   Shahrah-E-Ibn-E-Sina\n",
      "8  Shahrah-e-Bahadurabad  Shahrah-E-Bahadurabad\n",
      "9   Khayaban-e-Shamsheer   Khayaban-E-Shamsheer\n"
     ]
    }
   ],
   "source": [
    "# 1) Load the Accidents sheet if not already loaded\n",
    "try:\n",
    "    df_accidents\n",
    "except NameError:\n",
    "    df_accidents = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Accidents', engine='openpyxl')\n",
    "\n",
    "# 2) Check for any missing values in the 'Location' column\n",
    "missing_locations = df_accidents['Location'].isnull().sum()\n",
    "print(\"Missing Location values:\", missing_locations)\n",
    "\n",
    "# 3) Define a function to standardize location names (trim spaces, title case, etc.)\n",
    "def clean_location(loc):\n",
    "    if pd.isnull(loc):\n",
    "        return \"Unknown\"\n",
    "    # Remove extra spaces and convert to title case\n",
    "    return \" \".join(str(loc).strip().title().split())\n",
    "\n",
    "# 4) Apply the function to create a new column with cleaned location names\n",
    "df_accidents['Location_Cleaned'] = df_accidents['Location'].apply(clean_location)\n",
    "\n",
    "# 5) Show a sample of the original and cleaned location values for verification\n",
    "print(df_accidents[['Location', 'Location_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10c7f5-61f0-4de8-a025-314bb12f66f8",
   "metadata": {},
   "source": [
    "##### 17. SeverityAccidents: Standardize categories\t\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5fd482c7-6965-4c72-8caf-754943fefd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Severity values: 0\n",
      "   Severity Severity_Cleaned\n",
      "0     Minor            Minor\n",
      "1     Minor            Minor\n",
      "2    Severe           Severe\n",
      "3     Minor            Minor\n",
      "4    Severe           Severe\n",
      "5     Minor            Minor\n",
      "6  Moderate            Other\n",
      "7    Severe           Severe\n",
      "8  Moderate            Other\n",
      "9     Minor            Minor\n"
     ]
    }
   ],
   "source": [
    "# 1) Load the Accidents sheet (if not already loaded)\n",
    "try:\n",
    "    df_accidents\n",
    "except NameError:\n",
    "    df_accidents = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Accidents', engine='openpyxl')\n",
    "\n",
    "# 2) Check how many entries are missing in the Severity field\n",
    "missing_severity = df_accidents['Severity'].isnull().sum()\n",
    "print(\"Missing Severity values:\", missing_severity)\n",
    "\n",
    "# 3) Define a function to standardize severity categories\n",
    "def clean_severity(sev):\n",
    "    if pd.isnull(sev):\n",
    "        return \"Unknown\"\n",
    "    # Trim and convert to lower case for consistency\n",
    "    sev = str(sev).strip().lower()\n",
    "    if sev in [\"minor\", \"min\", \"low\"]:\n",
    "        return \"Minor\"\n",
    "    elif sev in [\"severe\", \"major\", \"high\"]:\n",
    "        return \"Severe\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# 4) Apply the function to create a new column with cleaned severity values\n",
    "df_accidents['Severity_Cleaned'] = df_accidents['Severity'].apply(clean_severity)\n",
    "\n",
    "# 5) Show a sample of original and cleaned Severity values\n",
    "print(df_accidents[['Severity', 'Severity_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49771d55-e3de-497d-9568-86f36d371950",
   "metadata": {},
   "source": [
    "##### 18. VehiclesInvolved: Check for negatives \t\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "15a17e84-9620-4cc0-a829-9a8e58e97ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives in VehiclesInvolved: Empty DataFrame\n",
      "Columns: [AccidentID, VehiclesInvolved]\n",
      "Index: []\n",
      "Final check (should be empty): Empty DataFrame\n",
      "Columns: [AccidentID, VehiclesInvolved_Cleaned]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the Accidents sheet now\n",
    "try:\n",
    "    df_accidents\n",
    "except NameError:\n",
    "    df_accidents = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Accidents', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for negative values in VehiclesInvolved now\n",
    "negatives = df_accidents[df_accidents['VehiclesInvolved'] < 0]\n",
    "print(\"Negatives in VehiclesInvolved:\", negatives[['AccidentID', 'VehiclesInvolved']])\n",
    "\n",
    "# 3) Replacing negative values with 1 now\n",
    "df_accidents['VehiclesInvolved_Cleaned'] = df_accidents['VehiclesInvolved'].apply(lambda x: 1 if x < 0 else x)\n",
    "\n",
    "# 4) Final check now to ensure no negatives remain\n",
    "final_negatives = df_accidents[df_accidents['VehiclesInvolved_Cleaned'] < 0]\n",
    "print(\"Final check (should be empty):\", final_negatives[['AccidentID', 'VehiclesInvolved_Cleaned']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf0bb8-b3b3-4ab0-a420-cffc7939800e",
   "metadata": {},
   "source": [
    "##### 19. ReportedAt: Ensure valid date-time\t\t\t\t\t\t\t \t\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b96f237f-03c4-4774-938a-f8322f9c6017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid ReportedAt entries: Empty DataFrame\n",
      "Columns: [AccidentID, ReportedAt]\n",
      "Index: []\n",
      "Cleaned ReportedAt sample:\n",
      "   AccidentID          ReportedAt  ReportedAt_Cleaned\n",
      "0           1 2025-02-20 15:15:34 2025-02-20 15:15:34\n",
      "1           2 2025-01-07 13:14:37 2025-01-07 13:14:37\n",
      "2           3 2025-02-03 15:03:38 2025-02-03 15:03:38\n",
      "3           4 2025-02-19 01:05:37 2025-02-19 01:05:37\n",
      "4           5 2025-02-15 15:15:27 2025-02-15 15:15:27\n",
      "5           6 2025-01-06 20:48:02 2025-01-06 20:48:02\n",
      "6           7 2025-02-01 01:52:06 2025-02-01 01:52:06\n",
      "7           8 2025-02-01 17:34:58 2025-02-01 17:34:58\n",
      "8           9 2025-01-13 14:28:48 2025-01-13 14:28:48\n",
      "9          10 2025-02-24 02:57:45 2025-02-24 02:57:45\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the Accidents sheet now\n",
    "try:\n",
    "    df_accidents\n",
    "except NameError:\n",
    "    df_accidents = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Accidents', engine='openpyxl')\n",
    "\n",
    "# 2) Checking ReportedAt for valid date-time now\n",
    "df_accidents['ReportedAt_Converted'] = pd.to_datetime(df_accidents['ReportedAt'], errors='coerce')\n",
    "invalid_reported_at = df_accidents[df_accidents['ReportedAt_Converted'].isna()]\n",
    "print(\"Invalid ReportedAt entries:\", invalid_reported_at[['AccidentID', 'ReportedAt']])\n",
    "\n",
    "# 3) Handling invalid ReportedAt values now by replacing with today's date\n",
    "df_accidents['ReportedAt_Cleaned'] = df_accidents['ReportedAt_Converted'].apply(\n",
    "    lambda x: pd.Timestamp('today') if pd.isna(x) else x\n",
    ")\n",
    "\n",
    "# 4) Final check now to ensure ReportedAt is valid\n",
    "print(\"Cleaned ReportedAt sample:\")\n",
    "print(df_accidents[['AccidentID', 'ReportedAt', 'ReportedAt_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303199d-a302-4fb8-96ed-1e89730ea67a",
   "metadata": {},
   "source": [
    "##### 20. WeatherID: Ensure data integrity\t\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9ba6a2b2-4f2a-473c-87e8-d8f28a609137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing WeatherID: 0\n",
      "Duplicate WeatherID count: 0\n",
      "Duplicate WeatherID rows:\n",
      "Empty DataFrame\n",
      "Columns: [WeatherID]\n",
      "Index: []\n",
      "Final duplicate count in WeatherID: 0\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the WeatherData sheet now\n",
    "try:\n",
    "    df_weather\n",
    "except NameError:\n",
    "    df_weather = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='WeatherData', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for missing WeatherID values now\n",
    "missing_weather_id = df_weather['WeatherID'].isnull().sum()\n",
    "print(\"Missing WeatherID:\", missing_weather_id)\n",
    "\n",
    "# 3) Checking for duplicate WeatherID values now\n",
    "duplicate_weather_id = df_weather['WeatherID'].duplicated().sum()\n",
    "print(\"Duplicate WeatherID count:\", duplicate_weather_id)\n",
    "\n",
    "# 4) Showing duplicate WeatherID rows now (if any)\n",
    "duplicate_rows = df_weather[df_weather['WeatherID'].duplicated()]\n",
    "print(\"Duplicate WeatherID rows:\")\n",
    "print(duplicate_rows[['WeatherID']])\n",
    "\n",
    "# 5) Removing duplicates now by keeping the first occurrence\n",
    "df_weather_cleaned = df_weather.drop_duplicates(subset=['WeatherID'], keep='first')\n",
    "\n",
    "# 6) Final check now to confirm WeatherID is unique\n",
    "final_duplicates = df_weather_cleaned['WeatherID'].duplicated().sum()\n",
    "print(\"Final duplicate count in WeatherID:\", final_duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c60a46-1c13-4570-8a6c-7005900a410f",
   "metadata": {},
   "source": [
    "##### 21. Temperature_C: Check for outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d495cb80-692f-4dec-9735-16da82e36805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 14.925 Q3: 30.525 IQR: 15.599999999999998\n",
      "Outliers in Temperature_C:\n",
      "Empty DataFrame\n",
      "Columns: [WeatherID, Temperature_C]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the WeatherData sheet now (if not already loaded)\n",
    "try:\n",
    "    df_weather\n",
    "except NameError:\n",
    "    df_weather = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='WeatherData', engine='openpyxl')\n",
    "\n",
    "# 2) Calculate Q1, Q3, and IQR for Temperature_C\n",
    "Q1 = df_weather['Temperature_C'].quantile(0.25)\n",
    "Q3 = df_weather['Temperature_C'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(\"Q1:\", Q1, \"Q3:\", Q3, \"IQR:\", IQR)\n",
    "\n",
    "# 3) Check for outliers: values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR\n",
    "outliers_temp = df_weather[\n",
    "    (df_weather['Temperature_C'] < Q1 - 1.5 * IQR) | \n",
    "    (df_weather['Temperature_C'] > Q3 + 1.5 * IQR)\n",
    "]\n",
    "print(\"Outliers in Temperature_C:\")\n",
    "print(outliers_temp[['WeatherID', 'Temperature_C']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56677b13-62e0-4b7b-bbd7-770bfab79c0d",
   "metadata": {},
   "source": [
    "##### 22. Humidity_Percent: Ensure 0–100 range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0b216af0-309f-4b04-bfc4-f3c8429c5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-range Humidity_Percent values:\n",
      "Empty DataFrame\n",
      "Columns: [WeatherID, Humidity_Percent]\n",
      "Index: []\n",
      "Sample of Humidity_Percent vs Cleaned values:\n",
      "   WeatherID  Humidity_Percent  Humidity_Percent_Cleaned\n",
      "0          1                40                        40\n",
      "1          2                48                        48\n",
      "2          3                59                        59\n",
      "3          4                41                        41\n",
      "4          5                27                        27\n",
      "5          6                57                        57\n",
      "6          7                31                        31\n",
      "7          8                56                        56\n",
      "8          9                82                        82\n",
      "9         10                90                        90\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the WeatherData sheet now (if not already loaded)\n",
    "try:\n",
    "    df_weather\n",
    "except NameError:\n",
    "    df_weather = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='WeatherData', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for Humidity_Percent values that are not between 0 and 100\n",
    "out_of_range_humidity = df_weather[(df_weather['Humidity_Percent'] < 0) | (df_weather['Humidity_Percent'] > 100)]\n",
    "print(\"Out-of-range Humidity_Percent values:\")\n",
    "print(out_of_range_humidity[['WeatherID', 'Humidity_Percent']])\n",
    "\n",
    "# 3) Replacing out-of-range values by clipping them to the 0-100 range\n",
    "df_weather['Humidity_Percent_Cleaned'] = df_weather['Humidity_Percent'].clip(lower=0, upper=100)\n",
    "\n",
    "# 4) Final check to compare original vs cleaned values\n",
    "print(\"Sample of Humidity_Percent vs Cleaned values:\")\n",
    "print(df_weather[['WeatherID', 'Humidity_Percent', 'Humidity_Percent_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb37d51-8cdd-4893-b6e3-b4f6ee392de6",
   "metadata": {},
   "source": [
    "##### 23. Condition: Standardize values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9cbd3f3c-c85e-4ff1-96b6-9e35cb34ab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Condition values: 0\n",
      "   WeatherID Condition Condition_Cleaned\n",
      "0          1    Cloudy            Cloudy\n",
      "1          2    Cloudy            Cloudy\n",
      "2          3    Cloudy            Cloudy\n",
      "3          4     Sunny             Sunny\n",
      "4          5    Cloudy            Cloudy\n",
      "5          6     Sunny             Sunny\n",
      "6          7    Stormy            Stormy\n",
      "7          8     Rainy             Rainy\n",
      "8          9    Cloudy            Cloudy\n",
      "9         10     Rainy             Rainy\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the WeatherData sheet now (if not already loaded)\n",
    "try:\n",
    "    df_weather\n",
    "except NameError:\n",
    "    df_weather = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='WeatherData', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for missing values in Condition\n",
    "missing_condition = df_weather['Condition'].isnull().sum()\n",
    "print(\"Missing Condition values:\", missing_condition)\n",
    "\n",
    "# 3) Define a function to standardize weather condition values\n",
    "def clean_condition(cond):\n",
    "    if pd.isnull(cond):\n",
    "        return \"Unknown\"\n",
    "    # Clean up string: trim, lower then title-case for consistency\n",
    "    cond = str(cond).strip().lower()\n",
    "    # Optionally map similar words to a standard value\n",
    "    if \"clear\" in cond or \"sunny\" in cond:\n",
    "        return \"Sunny\"\n",
    "    elif \"cloud\" in cond:\n",
    "        return \"Cloudy\"\n",
    "    elif \"rain\" in cond:\n",
    "        return \"Rainy\"\n",
    "    elif \"snow\" in cond:\n",
    "        return \"Snowy\"\n",
    "    elif \"fog\" in cond:\n",
    "        return \"Foggy\"\n",
    "    else:\n",
    "        return cond.title()\n",
    "\n",
    "# 4) Apply the cleaning function to create a new standardized column\n",
    "df_weather['Condition_Cleaned'] = df_weather['Condition'].apply(clean_condition)\n",
    "\n",
    "# 5) Final check: show a sample of original vs. cleaned Condition values\n",
    "print(df_weather[['WeatherID', 'Condition', 'Condition_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff4920-c043-4919-ab41-d55805015b85",
   "metadata": {},
   "source": [
    "##### 24. Timestamp: Validate datetime format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9e80a8f6-7c21-48d2-aa0d-2906d9f59c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Timestamp entries:\n",
      "Empty DataFrame\n",
      "Columns: [WeatherID, Timestamp]\n",
      "Index: []\n",
      "Sample of cleaned Timestamps:\n",
      "   WeatherID           Timestamp   Timestamp_Cleaned\n",
      "0          1 2025-01-08 05:19:23 2025-01-08 05:19:23\n",
      "1          2 2025-02-22 11:50:40 2025-02-22 11:50:40\n",
      "2          3 2025-02-20 05:23:27 2025-02-20 05:23:27\n",
      "3          4 2025-01-09 09:43:52 2025-01-09 09:43:52\n",
      "4          5 2025-01-29 17:16:29 2025-01-29 17:16:29\n",
      "5          6 2025-02-15 12:51:23 2025-02-15 12:51:23\n",
      "6          7 2025-02-13 23:39:08 2025-02-13 23:39:08\n",
      "7          8 2025-02-13 06:54:31 2025-02-13 06:54:31\n",
      "8          9 2025-01-28 22:01:41 2025-01-28 22:01:41\n",
      "9         10 2025-01-03 13:59:53 2025-01-03 13:59:53\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the WeatherData sheet now (if not already loaded)\n",
    "try:\n",
    "    df_weather\n",
    "except NameError:\n",
    "    df_weather = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='WeatherData', engine='openpyxl')\n",
    "\n",
    "# 2) Converting the Timestamp column to datetime to validate its format\n",
    "df_weather['Timestamp_Converted'] = pd.to_datetime(df_weather['Timestamp'], errors='coerce')\n",
    "\n",
    "# 3) Checking for any invalid timestamps (those that couldn't be converted)\n",
    "invalid_ts = df_weather[df_weather['Timestamp_Converted'].isna()]\n",
    "print(\"Invalid Timestamp entries:\")\n",
    "print(invalid_ts[['WeatherID', 'Timestamp']])\n",
    "\n",
    "# 4) Replacing invalid timestamps with today's date as a fallback\n",
    "df_weather['Timestamp_Cleaned'] = df_weather['Timestamp_Converted'].fillna(pd.Timestamp('today'))\n",
    "\n",
    "# 5) Final check: show a sample of original vs. cleaned Timestamps\n",
    "print(\"Sample of cleaned Timestamps:\")\n",
    "print(df_weather[['WeatherID', 'Timestamp', 'Timestamp_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af47cfb-b2dc-4bbe-8953-a52bf3af56f0",
   "metadata": {},
   "source": [
    "##### 25. IncidentID: Ensure uniqueness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4a881d76-7150-4aae-9e75-32f1bfecf402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing IncidentID values: 0\n",
      "Duplicate IncidentID count: 0\n",
      "Duplicate IncidentID rows:\n",
      "Empty DataFrame\n",
      "Columns: [IncidentID, Type]\n",
      "Index: []\n",
      "Final duplicate count for IncidentID: 0\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the TrafficIncidents sheet now\n",
    "try:\n",
    "    df_incidents\n",
    "except NameError:\n",
    "    df_incidents = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='TrafficIncidents', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for missing IncidentID values now\n",
    "missing_incident_ids = df_incidents['IncidentID'].isnull().sum()\n",
    "print(\"Missing IncidentID values:\", missing_incident_ids)\n",
    "\n",
    "# 3) Checking for duplicate IncidentID values now\n",
    "duplicate_incident_ids = df_incidents['IncidentID'].duplicated().sum()\n",
    "print(\"Duplicate IncidentID count:\", duplicate_incident_ids)\n",
    "\n",
    "# 4) Showing duplicate IncidentID rows now\n",
    "duplicate_rows = df_incidents[df_incidents['IncidentID'].duplicated()]\n",
    "print(\"Duplicate IncidentID rows:\")\n",
    "print(duplicate_rows[['IncidentID', 'Type']])\n",
    "\n",
    "# 5) Dropping duplicates now and keeping the first occurrence\n",
    "df_incidents_cleaned = df_incidents.drop_duplicates(subset=['IncidentID'], keep='first')\n",
    "\n",
    "# 6) Final check now to confirm IncidentID is unique\n",
    "final_duplicate_check = df_incidents_cleaned['IncidentID'].duplicated().sum()\n",
    "print(\"Final duplicate count for IncidentID:\", final_duplicate_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd88ed-9995-4e3b-9dee-ea5836aad49b",
   "metadata": {},
   "source": [
    "##### 26. Type: Group and standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e05da87a-27cf-4567-8539-be70756b153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing incident types: 0\n",
      "                Type       Type_Cleaned\n",
      "0  Vehicle Breakdown  Vehicle Breakdown\n",
      "1  Vehicle Breakdown  Vehicle Breakdown\n",
      "2          Road Rage          Road Rage\n",
      "3  Vehicle Breakdown  Vehicle Breakdown\n",
      "4          Road Rage          Road Rage\n",
      "5        Hit and Run              Other\n",
      "6          Road Rage          Road Rage\n",
      "7  Vehicle Breakdown  Vehicle Breakdown\n",
      "8        Hit and Run              Other\n",
      "9  Vehicle Breakdown  Vehicle Breakdown\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the TrafficIncidents sheet now (if not already loaded)\n",
    "try:\n",
    "    df_incidents\n",
    "except NameError:\n",
    "    df_incidents = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='TrafficIncidents', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for missing incident types now\n",
    "missing_types = df_incidents['Type'].isnull().sum()\n",
    "print(\"Missing incident types:\", missing_types)\n",
    "\n",
    "# 3) Defining a function to group and standardize incident types\n",
    "def clean_incident_type(itype):\n",
    "    if pd.isnull(itype):\n",
    "        return \"Unknown\"\n",
    "    itype = str(itype).strip().lower()\n",
    "    if \"breakdown\" in itype:\n",
    "        return \"Vehicle Breakdown\"\n",
    "    elif \"road rage\" in itype:\n",
    "        return \"Road Rage\"\n",
    "    elif \"accident\" in itype:\n",
    "        return \"Accident\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# 4) Applying the function to create a new column with standardized types\n",
    "df_incidents['Type_Cleaned'] = df_incidents['Type'].apply(clean_incident_type)\n",
    "\n",
    "# 5) Final check: preview original vs. cleaned incident types\n",
    "print(df_incidents[['Type', 'Type_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0310d6c-52b0-4431-97cd-329cbc7c4fbd",
   "metadata": {},
   "source": [
    "##### 27. Location: Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "809026b7-a89d-434d-b4bd-fa2c682616e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing incident locations: 0\n",
      "                Location       Location_Cleaned\n",
      "0       Shahrah-e-Faisal       Shahrah-E-Faisal\n",
      "1         MA Jinnah Road         Ma Jinnah Road\n",
      "2     II Chundrigar Road     Ii Chundrigar Road\n",
      "3        University Road        University Road\n",
      "4     Rashid Minhas Road     Rashid Minhas Road\n",
      "5     Shahrah-e-Pakistan     Shahrah-E-Pakistan\n",
      "6           Korangi Road           Korangi Road\n",
      "7     Shahrah-e-Quaideen     Shahrah-E-Quaideen\n",
      "8  Shaheed-e-Millat Road  Shaheed-E-Millat Road\n",
      "9      Shahrah-e-Gulberg      Shahrah-E-Gulberg\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the TrafficIncidents sheet now (if not already loaded)\n",
    "try:\n",
    "    df_incidents\n",
    "except NameError:\n",
    "    df_incidents = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='TrafficIncidents', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for missing values in 'Location'\n",
    "missing_locations = df_incidents['Location'].isnull().sum()\n",
    "print(\"Missing incident locations:\", missing_locations)\n",
    "\n",
    "# 3) Defining a function to standardize the location names\n",
    "def clean_location(loc):\n",
    "    if pd.isnull(loc):\n",
    "        return \"Unknown\"\n",
    "    return \" \".join(str(loc).strip().title().split())\n",
    "\n",
    "# 4) Applying the function to create a cleaned location column\n",
    "df_incidents['Location_Cleaned'] = df_incidents['Location'].apply(clean_location)\n",
    "\n",
    "# 5) Final check: Show sample of original vs. cleaned location values\n",
    "print(df_incidents[['Location', 'Location_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28fb49-e818-4a2b-b41d-743a694408d9",
   "metadata": {},
   "source": [
    "##### 28. ReportedAt: Ensure consistent datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "699468c4-32d1-400b-889f-a826b593401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid ReportedAt entries:\n",
      "Empty DataFrame\n",
      "Columns: [IncidentID, ReportedAt]\n",
      "Index: []\n",
      "Sample of original vs. cleaned ReportedAt:\n",
      "           ReportedAt  ReportedAt_Cleaned\n",
      "0 2025-02-15 14:46:00 2025-02-15 14:46:00\n",
      "1 2025-02-23 18:29:41 2025-02-23 18:29:41\n",
      "2 2025-01-15 23:55:50 2025-01-15 23:55:50\n",
      "3 2025-02-05 22:04:32 2025-02-05 22:04:32\n",
      "4 2025-01-15 19:18:05 2025-01-15 19:18:05\n",
      "5 2025-01-20 00:55:24 2025-01-20 00:55:24\n",
      "6 2025-01-30 07:56:37 2025-01-30 07:56:37\n",
      "7 2025-01-31 08:08:58 2025-01-31 08:08:58\n",
      "8 2025-03-03 00:41:46 2025-03-03 00:41:46\n",
      "9 2025-02-17 06:04:21 2025-02-17 06:04:21\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the TrafficIncidents sheet now (if not already loaded)\n",
    "try:\n",
    "    df_incidents\n",
    "except NameError:\n",
    "    df_incidents = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='TrafficIncidents', engine='openpyxl')\n",
    "\n",
    "# 2) Converting 'ReportedAt' to datetime format\n",
    "df_incidents['ReportedAt_Converted'] = pd.to_datetime(df_incidents['ReportedAt'], errors='coerce')\n",
    "\n",
    "# 3) Checking for any invalid date-time entries\n",
    "invalid_reported = df_incidents[df_incidents['ReportedAt_Converted'].isna()]\n",
    "print(\"Invalid ReportedAt entries:\")\n",
    "print(invalid_reported[['IncidentID', 'ReportedAt']])\n",
    "\n",
    "# 4) Replacing invalid date-times with today's timestamp\n",
    "df_incidents['ReportedAt_Cleaned'] = df_incidents['ReportedAt_Converted'].fillna(pd.Timestamp('today'))\n",
    "\n",
    "# 5) Final check: preview original vs. cleaned ReportedAt values\n",
    "print(\"Sample of original vs. cleaned ReportedAt:\")\n",
    "print(df_incidents[['ReportedAt', 'ReportedAt_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88bae5-5ae6-45b0-9d3a-b59bcafa6e96",
   "metadata": {},
   "source": [
    "##### 29. Location ID: Ensure data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0eba03a3-269f-47fe-94e3-836a33f2087e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Location ID values: 0\n",
      "Duplicate Location ID values: 0\n",
      "Duplicate rows in Location ID:\n",
      "Empty DataFrame\n",
      "Columns: [Location ID, Road Name]\n",
      "Index: []\n",
      "Final duplicate count in Location ID: 0\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the Location sheet now (if it's not already loaded)\n",
    "try:\n",
    "    df_location\n",
    "except NameError:\n",
    "    df_location = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Location', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for missing values in \"Location ID\" now\n",
    "missing_location_ids = df_location['Location ID'].isnull().sum()\n",
    "print(f\"Missing Location ID values: {missing_location_ids}\")\n",
    "\n",
    "# 3) Checking for duplicate Location ID values now\n",
    "duplicate_location_ids = df_location['Location ID'].duplicated().sum()\n",
    "print(f\"Duplicate Location ID values: {duplicate_location_ids}\")\n",
    "\n",
    "# 4) Showing rows with duplicate Location IDs now\n",
    "duplicate_rows = df_location[df_location['Location ID'].duplicated()]\n",
    "print(\"Duplicate rows in Location ID:\")\n",
    "print(duplicate_rows[['Location ID', 'Road Name']])\n",
    "\n",
    "# 5) Removing duplicates now (keeping the first occurrence)\n",
    "df_location_cleaned = df_location.drop_duplicates(subset=['Location ID'], keep='first')\n",
    "\n",
    "# 6) Final check now to confirm Location ID is unique\n",
    "final_duplicate_check = df_location_cleaned['Location ID'].duplicated().sum()\n",
    "print(f\"Final duplicate count in Location ID: {final_duplicate_check}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808af4b-379d-44d7-a8f6-6f5901dbc4e5",
   "metadata": {},
   "source": [
    "##### 30. Road Name: Standardize names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b83e255e-ed87-4a8a-af7c-37766257c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Road Name values: 0\n",
      "               Road Name      Road Name_Cleaned\n",
      "0       Shahrah-e-Faisal       Shahrah-E-Faisal\n",
      "1         MA Jinnah Road         Ma Jinnah Road\n",
      "2     II Chundrigar Road     Ii Chundrigar Road\n",
      "3        University Road        University Road\n",
      "4     Rashid Minhas Road     Rashid Minhas Road\n",
      "5     Shahrah-e-Pakistan     Shahrah-E-Pakistan\n",
      "6           Korangi Road           Korangi Road\n",
      "7     Shahrah-e-Quaideen     Shahrah-E-Quaideen\n",
      "8  Shaheed-e-Millat Road  Shaheed-E-Millat Road\n",
      "9       Sunset Boulevard       Sunset Boulevard\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the Location sheet now (if not already loaded)\n",
    "try:\n",
    "    df_location\n",
    "except NameError:\n",
    "    df_location = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Location', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for missing values in \"Road Name\"\n",
    "missing_road_name = df_location['Road Name'].isnull().sum()\n",
    "print(\"Missing Road Name values:\", missing_road_name)\n",
    "\n",
    "# 3) Defining a function to standardize road names (trim spaces, convert to title case)\n",
    "def clean_road_name(road):\n",
    "    if pd.isnull(road):\n",
    "        return \"Unknown\"\n",
    "    return \" \".join(str(road).strip().title().split())\n",
    "\n",
    "# 4) Applying the function to create a new column with cleaned road names\n",
    "df_location['Road Name_Cleaned'] = df_location['Road Name'].apply(clean_road_name)\n",
    "\n",
    "# 5) Final check now: preview original vs. cleaned Road Name values\n",
    "print(df_location[['Road Name', 'Road Name_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5f7db-b053-4d0c-934a-794be79ad91e",
   "metadata": {},
   "source": [
    "##### 31. Area/Location: Format location info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "267deb1d-fea7-40bd-9778-ddee3a3dee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Area/Location values: 0\n",
      "                          Area/Location                 Area/Location_Cleaned\n",
      "0  Jinnah International Airport to PIDC  Jinnah International Airport To Pidc\n",
      "1                    Saddar to Kharadar                    Saddar To Kharadar\n",
      "2             Tower to Merewether Tower             Tower To Merewether Tower\n",
      "3       NED University to Hassan Square       Ned University To Hassan Square\n",
      "4          Gulshan to Gulistan-e-Jauhar          Gulshan To Gulistan-E-Jauhar\n",
      "5            Sohrab Goth to Liaquatabad            Sohrab Goth To Liaquatabad\n",
      "6        Korangi Crossing to Qayyumabad        Korangi Crossing To Qayyumabad\n",
      "7                       Clifton to NIPA                       Clifton To Nipa\n",
      "8              Nursery to KDA Chowrangi              Nursery To Kda Chowrangi\n",
      "9                           DHA Phase 2                           Dha Phase 2\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the Location sheet now (if not already loaded)\n",
    "try:\n",
    "    df_location\n",
    "except NameError:\n",
    "    df_location = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Location', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for missing values in the 'Area/Location' column\n",
    "missing_area = df_location['Area/Location'].isnull().sum()\n",
    "print(\"Missing Area/Location values:\", missing_area)\n",
    "\n",
    "# 3) Defining a function to clean and format location info (trim spaces, convert to title case)\n",
    "def clean_area_location(loc):\n",
    "    if pd.isnull(loc):\n",
    "        return \"Unknown\"\n",
    "    return \" \".join(str(loc).strip().title().split())\n",
    "\n",
    "# 4) Applying the function to create a new cleaned column\n",
    "df_location['Area/Location_Cleaned'] = df_location['Area/Location'].apply(clean_area_location)\n",
    "\n",
    "# 5) Final check: Preview original vs. cleaned Area/Location values\n",
    "print(df_location[['Area/Location', 'Area/Location_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fbd32-24ee-4a5e-b37f-fb8a6325483e",
   "metadata": {},
   "source": [
    "##### 32. FlowID: Ensure uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cff0fbfd-8c1c-4a34-92cd-e59582a80ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing FlowID values: 0\n",
      "Duplicate FlowID count: 0\n",
      "Final duplicate check for FlowID: 0\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the TrafficFlow sheet now\n",
    "try:\n",
    "    df_traffic_flow\n",
    "except NameError:\n",
    "    df_traffic_flow = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='TrafficFlow', engine='openpyxl')\n",
    "\n",
    "# 2) Checking for missing FlowID values now\n",
    "missing_flow_ids = df_traffic_flow['FlowID'].isnull().sum()\n",
    "print(\"Missing FlowID values:\", missing_flow_ids)\n",
    "\n",
    "# 3) Checking for duplicate FlowID values now\n",
    "duplicate_flow_ids = df_traffic_flow['FlowID'].duplicated().sum()\n",
    "print(\"Duplicate FlowID count:\", duplicate_flow_ids)\n",
    "\n",
    "# 4) Dropping duplicates now and keeping the first occurrence\n",
    "df_traffic_flow_cleaned = df_traffic_flow.drop_duplicates(subset=['FlowID'], keep='first')\n",
    "\n",
    "# 5) Final check now to ensure no duplicates remain\n",
    "final_duplicate_check = df_traffic_flow_cleaned['FlowID'].duplicated().sum()\n",
    "print(\"Final duplicate check for FlowID:\", final_duplicate_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9deda7-540f-4f14-986b-12e11dc569df",
   "metadata": {},
   "source": [
    "##### 33. Location ID: Referential integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "142e99a1-49bf-4a5f-b87c-5e12f9bd41b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Location IDs in TrafficFlow (not in Location sheet):\n",
      "Empty DataFrame\n",
      "Columns: [FlowID, Location ID]\n",
      "Index: []\n",
      "Rows with invalid Location ID after cleaning:\n",
      "Empty DataFrame\n",
      "Columns: [FlowID, Location ID_Cleaned]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the TrafficFlow sheet now (if not already loaded)\n",
    "try:\n",
    "    df_traffic_flow\n",
    "except NameError:\n",
    "    df_traffic_flow = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='TrafficFlow', engine='openpyxl')\n",
    "\n",
    "# 2) Loading the Location sheet now (if not already loaded)\n",
    "try:\n",
    "    df_location\n",
    "except NameError:\n",
    "    df_location = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='Location', engine='openpyxl')\n",
    "\n",
    "# 3) Checking which Location IDs in TrafficFlow aren't in the Location sheet\n",
    "invalid_location_ids = df_traffic_flow[~df_traffic_flow['Location ID'].isin(df_location['Location ID'])]\n",
    "print(\"Invalid Location IDs in TrafficFlow (not in Location sheet):\")\n",
    "print(invalid_location_ids[['FlowID', 'Location ID']])\n",
    "\n",
    "# 4) Replacing invalid Location IDs with a default value (-1)\n",
    "df_traffic_flow['Location ID_Cleaned'] = df_traffic_flow['Location ID'].apply(\n",
    "    lambda x: x if x in set(df_location['Location ID']) else -1\n",
    ")\n",
    "\n",
    "# 5) Final check: show rows with invalid Location IDs after cleaning\n",
    "invalid_after = df_traffic_flow[df_traffic_flow['Location ID_Cleaned'] == -1]\n",
    "print(\"Rows with invalid Location ID after cleaning:\")\n",
    "print(invalid_after[['FlowID', 'Location ID_Cleaned']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ab120-563c-4da5-a32e-874862115697",
   "metadata": {},
   "source": [
    "##### 34. VehicleCount: Detect outliers and negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "61913886-3178-4640-bf30-a047ed2f26a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative VehicleCount values: Empty DataFrame\n",
      "Columns: [FlowID, VehicleCount]\n",
      "Index: []\n",
      "Q1: 302.25 Q3: 788.5 IQR: 486.25\n",
      "Outliers in VehicleCount: Empty DataFrame\n",
      "Columns: [FlowID, VehicleCount]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 1) Check for negative VehicleCount values now\n",
    "negatives = df_traffic_flow[df_traffic_flow['VehicleCount'] < 0]\n",
    "print(\"Negative VehicleCount values:\", negatives[['FlowID', 'VehicleCount']])\n",
    "\n",
    "# 2) Calculate Q1, Q3, and IQR for VehicleCount now\n",
    "Q1 = df_traffic_flow['VehicleCount'].quantile(0.25)\n",
    "Q3 = df_traffic_flow['VehicleCount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(\"Q1:\", Q1, \"Q3:\", Q3, \"IQR:\", IQR)\n",
    "\n",
    "# 3) Detect outliers now: values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR\n",
    "outliers = df_traffic_flow[(df_traffic_flow['VehicleCount'] < Q1 - 1.5 * IQR) | \n",
    "                           (df_traffic_flow['VehicleCount'] > Q3 + 1.5 * IQR)]\n",
    "print(\"Outliers in VehicleCount:\", outliers[['FlowID', 'VehicleCount']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16a65e-0d37-407d-92e5-70a37e71dfc6",
   "metadata": {},
   "source": [
    "##### 35. Timestamp: Ensure valid datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a8678d62-4dec-4166-ad28-cecce6ff4e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with invalid Timestamp values:\n",
      "Empty DataFrame\n",
      "Columns: [FlowID, Timestamp]\n",
      "Index: []\n",
      "Sample of original vs. cleaned Timestamps:\n",
      "   FlowID           Timestamp   Timestamp_Cleaned\n",
      "0       1 2025-02-15 22:08:31 2025-02-15 22:08:31\n",
      "1       2 2025-02-05 18:12:09 2025-02-05 18:12:09\n",
      "2       3 2025-02-17 07:56:45 2025-02-17 07:56:45\n",
      "3       4 2025-02-28 05:24:57 2025-02-28 05:24:57\n",
      "4       5 2025-02-26 03:51:13 2025-02-26 03:51:13\n",
      "5       6 2025-01-29 13:06:11 2025-01-29 13:06:11\n",
      "6       7 2025-01-17 16:34:50 2025-01-17 16:34:50\n",
      "7       8 2025-01-28 03:56:05 2025-01-28 03:56:05\n",
      "8       9 2025-01-15 04:42:43 2025-01-15 04:42:43\n",
      "9      10 2025-01-06 02:52:23 2025-01-06 02:52:23\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading the TrafficFlow sheet now (if not already loaded)\n",
    "try:\n",
    "    df_traffic_flow\n",
    "except NameError:\n",
    "    df_traffic_flow = pd.read_excel(\"Traffic_updated.xlsx\", sheet_name='TrafficFlow', engine='openpyxl')\n",
    "\n",
    "# 2) Converting the Timestamp column to datetime\n",
    "df_traffic_flow['Timestamp_Converted'] = pd.to_datetime(df_traffic_flow['Timestamp'], errors='coerce')\n",
    "\n",
    "# 3) Checking for any invalid timestamps now (conversion failed)\n",
    "invalid_ts = df_traffic_flow[df_traffic_flow['Timestamp_Converted'].isna()]\n",
    "print(\"Rows with invalid Timestamp values:\")\n",
    "print(invalid_ts[['FlowID', 'Timestamp']])\n",
    "\n",
    "# 4) Handling invalid timestamps: replace with current timestamp if needed\n",
    "df_traffic_flow['Timestamp_Cleaned'] = df_traffic_flow['Timestamp_Converted'].fillna(pd.Timestamp('today'))\n",
    "\n",
    "# 5) Final check: preview a sample of original and cleaned Timestamp values\n",
    "print(\"Sample of original vs. cleaned Timestamps:\")\n",
    "print(df_traffic_flow[['FlowID', 'Timestamp', 'Timestamp_Cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa784bb-671a-4142-8c21-b1318b9f3c30",
   "metadata": {},
   "source": [
    "## FACT TABLE CLEANING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c482e-d533-45a3-b574-732c84960572",
   "metadata": {},
   "source": [
    "##### 1. Vehicle-id: Ensure referential integrity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8777023-0beb-47a9-9bba-d94acc288827",
   "metadata": {},
   "source": [
    "##### 2. Driver-id: Ensure referential integrity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57fff6-d53f-4474-bab9-292741cbbae2",
   "metadata": {},
   "source": [
    "##### 3. Location-id: Standardize & create location mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d19417-5579-4701-85d4-f6715f38a56c",
   "metadata": {},
   "source": [
    "##### 4. Incident-date: Extract from ReportedAt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c074410-431c-4729-85bf-09f711129f93",
   "metadata": {},
   "source": [
    "##### 5. Accident-date: Extract from ReportedAt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47889fc1-18ce-429b-a0af-39edb771e576",
   "metadata": {},
   "source": [
    "##### 6. Incident-time (hrs): Extract time from ReportedAt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8353e-4662-455c-8273-2dcfc2b82149",
   "metadata": {},
   "source": [
    "##### 7. Accident-time (hrs): Extract time from ReportedAt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e97223-111e-4fb9-baee-2bb42d091db1",
   "metadata": {},
   "source": [
    "##### 8. Incident-type: Standardize & encode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c693f-9bbd-4d93-be1f-6a24d0106731",
   "metadata": {},
   "source": [
    "##### 9. Accident-severity: Standardize severity (e.g., Minor/Severe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a913b99-f205-4864-9df8-0f69a5521ba8",
   "metadata": {},
   "source": [
    "##### 10. Accident-vehicles-involved: Ensure non-negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44064718-0777-4614-8e39-14fbf4063f71",
   "metadata": {},
   "source": [
    "##### 11. Road-visibility: Standardize categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb364e78-7b57-469f-9bbc-6fb57e5b0742",
   "metadata": {},
   "source": [
    "##### 12. Weather-condition: Standardize categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f129f-23ab-4b81-aeeb-e47a025a77bf",
   "metadata": {},
   "source": [
    "##### 13. Driver-age: Ensure range compliance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78873506-3929-402c-9058-ce6d6a122ac9",
   "metadata": {},
   "source": [
    "##### 14. Driver-experience: Fix if negative or too high\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c70f88d-17b2-4b46-8ab0-29ece61a905b",
   "metadata": {},
   "source": [
    "##### 15. Reason-recorded: Group similar causes; standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4041e-917e-4c49-ab91-1f6dbd44ca3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e045af-07b6-4677-8a3a-d93864b24575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15abd3-6d3e-43d8-ab79-df98e490c0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
